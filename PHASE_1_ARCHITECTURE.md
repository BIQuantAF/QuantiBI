# Phase 1 Architecture Diagram

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QuantiBI Frontend                           â”‚
â”‚                      (React + TypeScript)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ HTTP/JSON
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Express Backend                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Routes Layer                          â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ datasets.js (upload files)  [TO BE UPDATED]          â”‚  â”‚
â”‚  â”‚  â€¢ charts.js (generate charts)  [TO BE UPDATED]         â”‚  â”‚
â”‚  â”‚  â€¢ databases.js (manage connections)  [TO BE UPDATED]   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                       â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚      â–¼                  â–¼                  â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ S3 Service â”‚   â”‚DuckDB Svc â”‚   â”‚ Mongoose   â”‚            â”‚
â”‚  â”‚ (NEW âœ¨)   â”‚   â”‚ (NEW âœ¨)   â”‚   â”‚  Models    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                   â”‚
         â”‚                  â”‚                   â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     AWS     â”‚  â”‚   Local     â”‚  â”‚    MongoDB      â”‚
    â”‚    S3       â”‚  â”‚  File Systemâ”‚  â”‚                 â”‚
    â”‚  Bucket     â”‚  â”‚   /tmp/     â”‚  â”‚  Users, Datasetsâ”‚
    â”‚             â”‚  â”‚             â”‚  â”‚  Charts, Reportsâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Upload Flow (Phase 2)

```
User uploads file
       â”‚
       â–¼
Express Route: datasets.js
   POST /upload
       â”‚
       â”œâ”€ Extract file from request
       â”‚
       â”œâ”€ Validate file type (CSV, XLSX)
       â”‚
       â”œâ”€ Call s3Service.uploadFile()
       â”‚    â”‚
       â”‚    â”œâ”€ Generate unique S3 key (files/workspace-123/timestamp-random.csv)
       â”‚    â”‚
       â”‚    â”œâ”€ Upload buffer to S3
       â”‚    â”‚
       â”‚    â””â”€ Return s3Key & metadata
       â”‚
       â”œâ”€ Call duckdbService.detectSchema()
       â”‚    â”‚
       â”‚    â”œâ”€ Download file from S3 to /tmp/
       â”‚    â”‚
       â”‚    â”œâ”€ Parse CSV/XLSX
       â”‚    â”‚
       â”‚    â””â”€ Return column schema
       â”‚
       â”œâ”€ Save to Database model
       â”‚    {
       â”‚      workspace: id,
       â”‚      name: 'Sales Data',
       â”‚      type: 'CSV',
       â”‚      s3Key: 'files/workspace-123/...',
       â”‚      s3Bucket: 'quantibi-files-dev',
       â”‚      fileSize: 1024,
       â”‚      schema: [...]
       â”‚    }
       â”‚
       â””â”€ Return 201 with database record
```

## Query Flow (Phase 2)

```
User requests chart from dataset
       â”‚
       â–¼
Express Route: charts.js
   POST /generate
       â”‚
       â”œâ”€ Get dataset by ID
       â”‚
       â”œâ”€ Get s3Key from database record
       â”‚
       â”œâ”€ Generate SQL via OpenAI
       â”‚    (Based on natural language query)
       â”‚
       â”œâ”€ Call s3Service.downloadFileToTemp()
       â”‚    â”‚
       â”‚    â”œâ”€ Create /tmp/quantibi/ directory
       â”‚    â”‚
       â”‚    â”œâ”€ Download s3Key from S3
       â”‚    â”‚
       â”‚    â””â”€ Return local file path
       â”‚
       â”œâ”€ Call duckdbService.executeChartQuery()
       â”‚    â”‚
       â”‚    â”œâ”€ Open CSV/XLSX file
       â”‚    â”‚
       â”‚    â”œâ”€ Create virtual table in DuckDB
       â”‚    â”‚
       â”‚    â”œâ”€ Execute SQL query
       â”‚    â”‚
       â”‚    â””â”€ Return aggregated results
       â”‚
       â”œâ”€ Transform results to Chart.js format
       â”‚
       â”œâ”€ Call s3Service.cleanupLocalFile()
       â”‚    â”‚
       â”‚    â””â”€ Delete /tmp/quantibi/filename
       â”‚
       â””â”€ Return 200 with chart data
```

## File Organization

```
quantibi-backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ s3.js                    âœ¨ NEW - S3 operations
â”‚   â”‚   â”œâ”€â”€ duckdb.js               âœ¨ NEW - DuckDB queries
â”‚   â”‚   â”œâ”€â”€ bigquery.js             âœ… Existing (keep for power users)
â”‚   â”‚   â””â”€â”€ usage.js                âœ… Existing
â”‚   â”‚
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ datasets.js             ğŸ“ TO UPDATE - use S3
â”‚   â”‚   â”œâ”€â”€ charts.js               ğŸ“ TO UPDATE - use DuckDB
â”‚   â”‚   â””â”€â”€ databases.js            ğŸ“ TO UPDATE - remove file ops
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ Database.js             âœ… UPDATED - added s3* fields
â”‚   â”‚   â”œâ”€â”€ Chart.js                âœ… Existing
â”‚   â”‚   â””â”€â”€ Dataset.js              âœ… Existing
â”‚   â”‚
â”‚   â””â”€â”€ index.js                    âœ… Existing
â”‚
â”œâ”€â”€ uploads/                        âš ï¸  DEPRECATED (Phase 2)
â”‚   â””â”€â”€ file-*.csv                  (will migrate to S3)
â”‚
â”œâ”€â”€ temp/                           âœ¨ NEW (temporary storage)
â”‚   â””â”€â”€ duckdb-*.db
â”‚   â””â”€â”€ quantibi/
â”‚       â””â”€â”€ downloaded-files
â”‚
â”œâ”€â”€ package.json                    âœ… UPDATED - added deps
â”œâ”€â”€ .env                            âœ… UPDATED - added AWS vars
â””â”€â”€ .env.example                    ğŸ“ TO UPDATE - add AWS template
```

## Data Flow: End-to-End

```
SCENARIO: User uploads CSV, generates chart, downloads report

1. UPLOAD PHASE
   User selects file.csv
   â”‚
   â”œâ”€ Frontend: POST /api/datasets/upload (multipart/form-data)
   â”‚
   â”œâ”€ Backend:
   â”‚   â”œâ”€ s3Service.uploadFile(buffer, 'file.csv', workspaceId)
   â”‚   â”‚  â””â”€ S3 Bucket: files/workspace-123/1700000000-abc.csv âœ…
   â”‚   â”‚
   â”‚   â”œâ”€ duckdbService.detectSchema(temp/file.csv)
   â”‚   â”‚  â””â”€ Result: [{ name: 'Product', type: 'VARCHAR' }, ...]
   â”‚   â”‚
   â”‚   â””â”€ Database.create({ s3Key: '...', schema: [...] })
   â”‚
   â””â”€ Response: 201 Created { datasetId, schema, s3Key }

2. CHART GENERATION PHASE
   User: "Show sales by product"
   â”‚
   â”œâ”€ Frontend: POST /api/charts (query: "Show sales by product")
   â”‚
   â”œâ”€ Backend:
   â”‚   â”œâ”€ OpenAI generates SQL: "SELECT product, SUM(amount) FROM data GROUP BY product"
   â”‚   â”‚
   â”‚   â”œâ”€ s3Service.downloadFileToTemp('files/workspace-123/1700000000-abc.csv')
   â”‚   â”‚  â””â”€ Local: /tmp/quantibi/1700000000-abc.csv âœ…
   â”‚   â”‚
   â”‚   â”œâ”€ duckdbService.executeChartQuery(local_path, sql)
   â”‚   â”‚  â”œâ”€ Load CSV into DuckDB
   â”‚   â”‚  â”œâ”€ Execute: SELECT product, SUM(amount) FROM data GROUP BY product
   â”‚   â”‚  â””â”€ Result: [
   â”‚   â”‚         { product: 'Widget', sum: 5000 },
   â”‚   â”‚         { product: 'Gadget', sum: 3000 }
   â”‚   â”‚      ]
   â”‚   â”‚
   â”‚   â”œâ”€ Format for Chart.js (labels, datasets)
   â”‚   â”‚
   â”‚   â”œâ”€ s3Service.cleanupLocalFile(/tmp/quantibi/1700000000-abc.csv)
   â”‚   â”‚  â””â”€ Deleted âœ…
   â”‚   â”‚
   â”‚   â””â”€ Database.create({ title: '...', type: 'bar', data: {...} })
   â”‚
   â””â”€ Response: 201 Created { chartId, chart_data }

3. REPORT GENERATION PHASE (Future)
   User: "Generate AI report from these charts"
   â”‚
   â”œâ”€ Backend (async):
   â”‚   â”œâ”€ OpenAI: "Summarize these chart insights"
   â”‚   â”‚
   â”‚   â””â”€ Generate PDF:
   â”‚       â”œâ”€ Puppeteer renders HTML â†’ PDF
   â”‚       â”‚
   â”‚       â”œâ”€ s3Service.uploadFile(pdfBuffer, 'report.pdf')
   â”‚       â”‚  â””â”€ S3: files/workspace-123/report-1700000000.pdf âœ…
   â”‚       â”‚
   â”‚       â””â”€ Report.update({ pdfS3Key: '...', status: 'completed' })
   â”‚
   â””â”€ Frontend polls /api/reports/:id until status=completed
```

## Database Schema Changes

### Before (Phase 0)
```javascript
Database {
  filePath: 'uploads/file-123.csv',  // Local storage
  fileType: 'CSV'
}
```

### After (Phase 1)
```javascript
Database {
  filePath: 'uploads/file-123.csv',  // Keep for backward compat
  fileType: 'CSV',
  
  // NEW S3 fields
  s3Key: 'files/workspace-123/1700000000-abc.csv',
  s3Bucket: 'quantibi-files-dev',
  s3Url: 'https://s3.amazonaws.com/.../file.csv',
  fileSize: 1024
}
```

## Environment Variables

```env
# AWS S3 Configuration
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

# S3 Storage Organization
S3_BUCKET_NAME=quantibi-files-dev           # Bucket name
S3_TEMP_FOLDER=temp/                        # Temporary files folder
S3_FILES_FOLDER=files/                      # Persistent files folder
```

## Performance Characteristics

| Operation | Time | Notes |
|-----------|------|-------|
| Upload 10MB CSV to S3 | ~2-5s | Depends on network |
| Download 10MB from S3 to temp | ~2-5s | Depends on network |
| Parse 10MB CSV in DuckDB | ~100-200ms | Local processing |
| Execute simple aggregation | ~50-100ms | In-memory query |
| **Total for chart generation** | **~5-11s** | End-to-end |

vs BigQuery: **20-30s** (due to API calls)
vs In-Memory (old): **1-2s** (but doesn't scale to 100MB+)

## Scaling Strategy

1. **Immediate** (0-100K users)
   - S3 Standard tier
   - DuckDB in-process
   - Temp files cleaned up after 1 hour

2. **Medium** (100K-1M users)
   - S3 with intelligent-tiering
   - DuckDB on separate process pool
   - Lambda functions for async processing

3. **Large** (1M+ users)
   - S3 with lifecycle policies
   - DuckDB cluster or replace with Presto/Trino
   - Message queue for async jobs (SQS/RabbitMQ)
